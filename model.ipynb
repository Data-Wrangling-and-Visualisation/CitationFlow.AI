{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpandas\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mpd\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mfeature_extraction\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtext\u001b[39;00m \u001b[39mimport\u001b[39;00m TfidfVectorizer\n",
      "File \u001b[1;32mc:\\Program Files\\Python310\\lib\\site-packages\\pandas\\__init__.py:25\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpandas\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcompat\u001b[39;00m \u001b[39mimport\u001b[39;00m is_numpy_dev \u001b[39mas\u001b[39;00m _is_numpy_dev  \u001b[39m# pyright: ignore # noqa:F401\u001b[39;00m\n\u001b[0;32m     24\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 25\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39mpandas\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_libs\u001b[39;00m \u001b[39mimport\u001b[39;00m hashtable \u001b[39mas\u001b[39;00m _hashtable, lib \u001b[39mas\u001b[39;00m _lib, tslib \u001b[39mas\u001b[39;00m _tslib\n\u001b[0;32m     26\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mImportError\u001b[39;00m \u001b[39mas\u001b[39;00m _err:  \u001b[39m# pragma: no cover\u001b[39;00m\n\u001b[0;32m     27\u001b[0m     _module \u001b[39m=\u001b[39m _err\u001b[39m.\u001b[39mname\n",
      "File \u001b[1;32mc:\\Program Files\\Python310\\lib\\site-packages\\pandas\\_libs\\__init__.py:13\u001b[0m\n\u001b[0;32m      1\u001b[0m __all__ \u001b[39m=\u001b[39m [\n\u001b[0;32m      2\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mNaT\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m      3\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mNaTType\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mInterval\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m     10\u001b[0m ]\n\u001b[1;32m---> 13\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpandas\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_libs\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39minterval\u001b[39;00m \u001b[39mimport\u001b[39;00m Interval\n\u001b[0;32m     14\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpandas\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_libs\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtslibs\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[0;32m     15\u001b[0m     NaT,\n\u001b[0;32m     16\u001b[0m     NaTType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     21\u001b[0m     iNaT,\n\u001b[0;32m     22\u001b[0m )\n",
      "File \u001b[1;32minterval.pyx:1\u001b[0m, in \u001b[0;36minit pandas._libs.interval\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import joblib\n",
    "import ast\n",
    "\n",
    "# Load the dataset (assuming you've downloaded it from Kaggle)\n",
    "# Dataset: https://www.kaggle.com/datasets/spsayakpaul/arxiv-paper-abstracts?select=arxiv_data_210930-054931.csv\n",
    "df = pd.read_csv('arxiv_data_210930-054931.csv')\n",
    "\n",
    "df['terms'] = df['terms'].apply(ast.literal_eval)\n",
    "\n",
    "# Display basic info\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(\"Columns:\", df.columns.tolist())\n",
    "print(\"\\nSample titles:\")\n",
    "print(df['titles'].head())\n",
    "\n",
    "# Define our target topics and map arXiv categories to them\n",
    "topic_mapping = {\n",
    "    'cs.AI': 'AI',\n",
    "    'cs.LG': 'ML',\n",
    "    'cs.CL': 'AI',  # Computational Linguistics → AI\n",
    "    'cs.NE': 'Reinforcement Learning',  # Neural and Evolutionary Computing\n",
    "    'q-bio.QM': 'Medicine',  # Quantitative Methods in Biology\n",
    "    'q-bio.NC': 'Medicine',  # Neurons and Cognition\n",
    "    'physics.class-ph': 'Physics of motion',  # Classical Physics\n",
    "    'stat.ML': 'ML'  # Statistics → Machine Learning\n",
    "}\n",
    "def map_to_topic(terms):\n",
    "    for term in terms:\n",
    "        if term in topic_mapping:\n",
    "            return topic_mapping[term]\n",
    "    return None\n",
    "# Create our target variable by mapping categories\n",
    "\n",
    "df['topic'] = df['terms'].apply(map_to_topic)\n",
    "print(df['topic'].head(5))\n",
    "\n",
    "# Filter only the rows with our desired topics\n",
    "df = df[df['topic'].notna()]\n",
    "\n",
    "\n",
    "# Check class distribution\n",
    "print(\"\\nClass distribution:\")\n",
    "print(df['topic'].value_counts())\n",
    "\n",
    "# Balance the classes (optional but recommended)\n",
    "min_samples = min(df['topic'].value_counts())\n",
    "df = df.groupby('topic').apply(lambda x: x.sample(min_samples)).reset_index(drop=True)\n",
    "\n",
    "# Preprocess text data\n",
    "def preprocess_text(text):\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    # Simple preprocessing: lowercase and remove special chars\n",
    "    text = text.lower()\n",
    "    text = ''.join([c for c in text if c.isalnum() or c.isspace()])\n",
    "    return text\n",
    "\n",
    "df['processed_title'] = df['titles'].apply(preprocess_text)\n",
    "\n",
    "# Split data\n",
    "X = df['processed_title']\n",
    "y = df['topic']\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Create and train the model pipeline\n",
    "model = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(\n",
    "        stop_words='english',\n",
    "        ngram_range=(1, 2),  # Use unigrams and bigrams\n",
    "        max_features=10000,\n",
    "        min_df=5,\n",
    "        max_df=0.7\n",
    "    )),\n",
    "    ('clf', LogisticRegression(\n",
    "        max_iter=1000,\n",
    "        class_weight='balanced',\n",
    "        multi_class='multinomial',\n",
    "        solver='saga',\n",
    "        penalty='elasticnet',\n",
    "        l1_ratio=0.5  # Mix of L1 and L2 regularization\n",
    "    ))\n",
    "])\n",
    "\n",
    "# Train model\n",
    "print(\"\\nTraining model...\")\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate\n",
    "y_pred = model.predict(X_test)\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Save the model\n",
    "joblib.dump(model, 'arxiv_title_classifier.pkl')\n",
    "print(\"\\nModel saved as 'arxiv_title_classifier.pkl'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "BrokenProcessPool",
     "evalue": "A task has failed to un-serialize. Please ensure that the arguments of the function are all picklable.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31m_RemoteTraceback\u001b[0m                          Traceback (most recent call last)",
      "\u001b[1;31m_RemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"c:\\Program Files\\Python310\\lib\\site-packages\\joblib\\externals\\loky\\process_executor.py\", line 426, in _process_worker\n    call_item = call_queue.get(block=True, timeout=timeout)\n  File \"c:\\Program Files\\Python310\\lib\\multiprocessing\\queues.py\", line 122, in get\n    return _ForkingPickler.loads(res)\n  File \"c:\\Program Files\\Python310\\lib\\site-packages\\sklearn\\__init__.py\", line 73, in <module>\n    from .base import clone  # noqa: E402\n  File \"c:\\Program Files\\Python310\\lib\\site-packages\\sklearn\\base.py\", line 19, in <module>\n    from .utils._estimator_html_repr import _HTMLDocumentationLinkMixin, estimator_html_repr\n  File \"c:\\Program Files\\Python310\\lib\\site-packages\\sklearn\\utils\\__init__.py\", line 15, in <module>\n    from ._chunking import gen_batches, gen_even_slices\n  File \"c:\\Program Files\\Python310\\lib\\site-packages\\sklearn\\utils\\_chunking.py\", line 11, in <module>\n    from ._param_validation import Interval, validate_params\n  File \"c:\\Program Files\\Python310\\lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 17, in <module>\n    from .validation import _is_arraylike_not_scalar\n  File \"c:\\Program Files\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 21, in <module>\n    from ..utils._array_api import _asarray_with_order, _is_numpy_namespace, get_namespace\n  File \"c:\\Program Files\\Python310\\lib\\site-packages\\sklearn\\utils\\_array_api.py\", line 17, in <module>\n    from .fixes import parse_version\n  File \"c:\\Program Files\\Python310\\lib\\site-packages\\sklearn\\utils\\fixes.py\", line 20, in <module>\n    import pandas as pd\n  File \"c:\\Program Files\\Python310\\lib\\site-packages\\pandas\\__init__.py\", line 59, in <module>\n    from pandas.core.api import (\n  File \"c:\\Program Files\\Python310\\lib\\site-packages\\pandas\\core\\api.py\", line 1, in <module>\n    from pandas._libs import (\n  File \"c:\\Program Files\\Python310\\lib\\site-packages\\pandas\\_libs\\__init__.py\", line 18, in <module>\n    from pandas._libs.interval import Interval\n  File \"interval.pyx\", line 1, in init pandas._libs.interval\nValueError: numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mBrokenProcessPool\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[36], line 33\u001b[0m\n\u001b[0;32m     30\u001b[0m     plt\u001b[39m.\u001b[39mlegend(loc\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mbest\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     31\u001b[0m     plt\u001b[39m.\u001b[39mshow()\n\u001b[1;32m---> 33\u001b[0m plot_learning_curve(model, \u001b[39m\"\u001b[39;49m\u001b[39mLearning Curve\u001b[39;49m\u001b[39m\"\u001b[39;49m, X, y)\n\u001b[0;32m     35\u001b[0m \u001b[39m# 2. Plot Validation Curve for different regularization strengths\u001b[39;00m\n\u001b[0;32m     36\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodel_selection\u001b[39;00m \u001b[39mimport\u001b[39;00m validation_curve\n",
      "Cell \u001b[1;32mIn[36], line 7\u001b[0m, in \u001b[0;36mplot_learning_curve\u001b[1;34m(estimator, title, X, y, cv)\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mplot_learning_curve\u001b[39m(estimator, title, X, y, cv\u001b[39m=\u001b[39m\u001b[39m5\u001b[39m):\n\u001b[1;32m----> 7\u001b[0m     train_sizes, train_scores, test_scores \u001b[39m=\u001b[39m learning_curve(\n\u001b[0;32m      8\u001b[0m         estimator, X, y, cv\u001b[39m=\u001b[39;49mcv, n_jobs\u001b[39m=\u001b[39;49m\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m,\n\u001b[0;32m      9\u001b[0m         train_sizes\u001b[39m=\u001b[39;49mnp\u001b[39m.\u001b[39;49mlinspace(\u001b[39m0.1\u001b[39;49m, \u001b[39m1.0\u001b[39;49m, \u001b[39m5\u001b[39;49m),\n\u001b[0;32m     10\u001b[0m         scoring\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39maccuracy\u001b[39;49m\u001b[39m'\u001b[39;49m\n\u001b[0;32m     11\u001b[0m     )\n\u001b[0;32m     13\u001b[0m     plt\u001b[39m.\u001b[39mfigure(figsize\u001b[39m=\u001b[39m(\u001b[39m10\u001b[39m, \u001b[39m6\u001b[39m))\n\u001b[0;32m     14\u001b[0m     plt\u001b[39m.\u001b[39mtitle(title)\n",
      "File \u001b[1;32mc:\\Program Files\\Python310\\lib\\site-packages\\sklearn\\utils\\_param_validation.py:216\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    210\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    211\u001b[0m     \u001b[39mwith\u001b[39;00m config_context(\n\u001b[0;32m    212\u001b[0m         skip_parameter_validation\u001b[39m=\u001b[39m(\n\u001b[0;32m    213\u001b[0m             prefer_skip_nested_validation \u001b[39mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    214\u001b[0m         )\n\u001b[0;32m    215\u001b[0m     ):\n\u001b[1;32m--> 216\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    217\u001b[0m \u001b[39mexcept\u001b[39;00m InvalidParameterError \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    218\u001b[0m     \u001b[39m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[0;32m    219\u001b[0m     \u001b[39m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[0;32m    220\u001b[0m     \u001b[39m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[0;32m    221\u001b[0m     \u001b[39m# message to avoid confusion.\u001b[39;00m\n\u001b[0;32m    222\u001b[0m     msg \u001b[39m=\u001b[39m re\u001b[39m.\u001b[39msub(\n\u001b[0;32m    223\u001b[0m         \u001b[39mr\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mparameter of \u001b[39m\u001b[39m\\\u001b[39m\u001b[39mw+ must be\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    224\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mparameter of \u001b[39m\u001b[39m{\u001b[39;00mfunc\u001b[39m.\u001b[39m\u001b[39m__qualname__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m must be\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    225\u001b[0m         \u001b[39mstr\u001b[39m(e),\n\u001b[0;32m    226\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Program Files\\Python310\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:2085\u001b[0m, in \u001b[0;36mlearning_curve\u001b[1;34m(estimator, X, y, groups, train_sizes, cv, scoring, exploit_incremental_learning, n_jobs, pre_dispatch, verbose, shuffle, random_state, error_score, return_times, fit_params, params)\u001b[0m\n\u001b[0;32m   2082\u001b[0m     \u001b[39mfor\u001b[39;00m n_train_samples \u001b[39min\u001b[39;00m train_sizes_abs:\n\u001b[0;32m   2083\u001b[0m         train_test_proportions\u001b[39m.\u001b[39mappend((train[:n_train_samples], test))\n\u001b[1;32m-> 2085\u001b[0m results \u001b[39m=\u001b[39m parallel(\n\u001b[0;32m   2086\u001b[0m     delayed(_fit_and_score)(\n\u001b[0;32m   2087\u001b[0m         clone(estimator),\n\u001b[0;32m   2088\u001b[0m         X,\n\u001b[0;32m   2089\u001b[0m         y,\n\u001b[0;32m   2090\u001b[0m         scorer\u001b[39m=\u001b[39;49mscorer,\n\u001b[0;32m   2091\u001b[0m         train\u001b[39m=\u001b[39;49mtrain,\n\u001b[0;32m   2092\u001b[0m         test\u001b[39m=\u001b[39;49mtest,\n\u001b[0;32m   2093\u001b[0m         verbose\u001b[39m=\u001b[39;49mverbose,\n\u001b[0;32m   2094\u001b[0m         parameters\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m,\n\u001b[0;32m   2095\u001b[0m         fit_params\u001b[39m=\u001b[39;49mrouted_params\u001b[39m.\u001b[39;49mestimator\u001b[39m.\u001b[39;49mfit,\n\u001b[0;32m   2096\u001b[0m         score_params\u001b[39m=\u001b[39;49mrouted_params\u001b[39m.\u001b[39;49mscorer\u001b[39m.\u001b[39;49mscore,\n\u001b[0;32m   2097\u001b[0m         return_train_score\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m   2098\u001b[0m         error_score\u001b[39m=\u001b[39;49merror_score,\n\u001b[0;32m   2099\u001b[0m         return_times\u001b[39m=\u001b[39;49mreturn_times,\n\u001b[0;32m   2100\u001b[0m     )\n\u001b[0;32m   2101\u001b[0m     \u001b[39mfor\u001b[39;49;00m train, test \u001b[39min\u001b[39;49;00m train_test_proportions\n\u001b[0;32m   2102\u001b[0m )\n\u001b[0;32m   2103\u001b[0m _warn_or_raise_about_fit_failures(results, error_score)\n\u001b[0;32m   2104\u001b[0m results \u001b[39m=\u001b[39m _aggregate_score_dicts(results)\n",
      "File \u001b[1;32mc:\\Program Files\\Python310\\lib\\site-packages\\sklearn\\utils\\parallel.py:77\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     72\u001b[0m config \u001b[39m=\u001b[39m get_config()\n\u001b[0;32m     73\u001b[0m iterable_with_config \u001b[39m=\u001b[39m (\n\u001b[0;32m     74\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     75\u001b[0m     \u001b[39mfor\u001b[39;00m delayed_func, args, kwargs \u001b[39min\u001b[39;00m iterable\n\u001b[0;32m     76\u001b[0m )\n\u001b[1;32m---> 77\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__call__\u001b[39;49m(iterable_with_config)\n",
      "File \u001b[1;32mc:\\Program Files\\Python310\\lib\\site-packages\\joblib\\parallel.py:2007\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   2001\u001b[0m \u001b[39m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[0;32m   2002\u001b[0m \u001b[39m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[0;32m   2003\u001b[0m \u001b[39m# reaches the first `yield` statement. This starts the asynchronous\u001b[39;00m\n\u001b[0;32m   2004\u001b[0m \u001b[39m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[0;32m   2005\u001b[0m \u001b[39mnext\u001b[39m(output)\n\u001b[1;32m-> 2007\u001b[0m \u001b[39mreturn\u001b[39;00m output \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreturn_generator \u001b[39melse\u001b[39;00m \u001b[39mlist\u001b[39;49m(output)\n",
      "File \u001b[1;32mc:\\Program Files\\Python310\\lib\\site-packages\\joblib\\parallel.py:1650\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[1;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[0;32m   1647\u001b[0m     \u001b[39myield\u001b[39;00m\n\u001b[0;32m   1649\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend\u001b[39m.\u001b[39mretrieval_context():\n\u001b[1;32m-> 1650\u001b[0m         \u001b[39myield from\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_retrieve()\n\u001b[0;32m   1652\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mGeneratorExit\u001b[39;00m:\n\u001b[0;32m   1653\u001b[0m     \u001b[39m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[0;32m   1654\u001b[0m     \u001b[39m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[0;32m   1655\u001b[0m     \u001b[39m# the user if necessary.\u001b[39;00m\n\u001b[0;32m   1656\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_exception \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Program Files\\Python310\\lib\\site-packages\\joblib\\parallel.py:1754\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1747\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_wait_retrieval():\n\u001b[0;32m   1748\u001b[0m \n\u001b[0;32m   1749\u001b[0m     \u001b[39m# If the callback thread of a worker has signaled that its task\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m     \u001b[39m# triggered an exception, or if the retrieval loop has raised an\u001b[39;00m\n\u001b[0;32m   1751\u001b[0m     \u001b[39m# exception (e.g. `GeneratorExit`), exit the loop and surface the\u001b[39;00m\n\u001b[0;32m   1752\u001b[0m     \u001b[39m# worker traceback.\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_aborting:\n\u001b[1;32m-> 1754\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_raise_error_fast()\n\u001b[0;32m   1755\u001b[0m         \u001b[39mbreak\u001b[39;00m\n\u001b[0;32m   1757\u001b[0m     \u001b[39m# If the next job is not ready for retrieval yet, we just wait for\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m     \u001b[39m# async callbacks to progress.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Program Files\\Python310\\lib\\site-packages\\joblib\\parallel.py:1789\u001b[0m, in \u001b[0;36mParallel._raise_error_fast\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1785\u001b[0m \u001b[39m# If this error job exists, immediately raise the error by\u001b[39;00m\n\u001b[0;32m   1786\u001b[0m \u001b[39m# calling get_result. This job might not exists if abort has been\u001b[39;00m\n\u001b[0;32m   1787\u001b[0m \u001b[39m# called directly or if the generator is gc'ed.\u001b[39;00m\n\u001b[0;32m   1788\u001b[0m \u001b[39mif\u001b[39;00m error_job \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m-> 1789\u001b[0m     error_job\u001b[39m.\u001b[39;49mget_result(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtimeout)\n",
      "File \u001b[1;32mc:\\Program Files\\Python310\\lib\\site-packages\\joblib\\parallel.py:745\u001b[0m, in \u001b[0;36mBatchCompletionCallBack.get_result\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    739\u001b[0m backend \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mparallel\u001b[39m.\u001b[39m_backend\n\u001b[0;32m    741\u001b[0m \u001b[39mif\u001b[39;00m backend\u001b[39m.\u001b[39msupports_retrieve_callback:\n\u001b[0;32m    742\u001b[0m     \u001b[39m# We assume that the result has already been retrieved by the\u001b[39;00m\n\u001b[0;32m    743\u001b[0m     \u001b[39m# callback thread, and is stored internally. It's just waiting to\u001b[39;00m\n\u001b[0;32m    744\u001b[0m     \u001b[39m# be returned.\u001b[39;00m\n\u001b[1;32m--> 745\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_return_or_raise()\n\u001b[0;32m    747\u001b[0m \u001b[39m# For other backends, the main thread needs to run the retrieval step.\u001b[39;00m\n\u001b[0;32m    748\u001b[0m \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Program Files\\Python310\\lib\\site-packages\\joblib\\parallel.py:763\u001b[0m, in \u001b[0;36mBatchCompletionCallBack._return_or_raise\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    761\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    762\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstatus \u001b[39m==\u001b[39m TASK_ERROR:\n\u001b[1;32m--> 763\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_result\n\u001b[0;32m    764\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_result\n\u001b[0;32m    765\u001b[0m \u001b[39mfinally\u001b[39;00m:\n",
      "\u001b[1;31mBrokenProcessPool\u001b[0m: A task has failed to un-serialize. Please ensure that the arguments of the function are all picklable."
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score, log_loss\n",
    "from sklearn.model_selection import learning_curve\n",
    "\n",
    "# 1. Plot Learning Curve\n",
    "def plot_learning_curve(estimator, title, X, y, cv=5):\n",
    "    train_sizes, train_scores, test_scores = learning_curve(\n",
    "        estimator, X, y, cv=cv, n_jobs=-1,\n",
    "        train_sizes=np.linspace(0.1, 1.0, 5),\n",
    "        scoring='accuracy'\n",
    "    )\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Training examples\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    \n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "    \n",
    "    plt.grid()\n",
    "    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
    "                     train_scores_mean + train_scores_std, alpha=0.1, color=\"r\")\n",
    "    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
    "                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n",
    "    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\", label=\"Training score\")\n",
    "    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\", label=\"Cross-validation score\")\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.show()\n",
    "\n",
    "plot_learning_curve(model, \"Learning Curve\", X, y)\n",
    "\n",
    "# 2. Plot Validation Curve for different regularization strengths\n",
    "from sklearn.model_selection import validation_curve\n",
    "\n",
    "param_range = np.logspace(-4, 4, 10)\n",
    "train_scores, test_scores = validation_curve(\n",
    "    model.named_steps['clf'], \n",
    "    model.named_steps['tfidf'].transform(X),\n",
    "    y,\n",
    "    param_name=\"C\",\n",
    "    param_range=param_range,\n",
    "    cv=5,\n",
    "    scoring=\"accuracy\",\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.title(\"Validation Curve with Logistic Regression\")\n",
    "plt.xlabel(\"Regularization Strength (C)\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.ylim(0.0, 1.1)\n",
    "lw = 2\n",
    "\n",
    "train_scores_mean = np.mean(train_scores, axis=1)\n",
    "train_scores_std = np.std(train_scores, axis=1)\n",
    "test_scores_mean = np.mean(test_scores, axis=1)\n",
    "test_scores_std = np.std(test_scores, axis=1)\n",
    "\n",
    "plt.semilogx(param_range, train_scores_mean, label=\"Training score\",\n",
    "             color=\"darkorange\", lw=lw)\n",
    "plt.fill_between(param_range, train_scores_mean - train_scores_std,\n",
    "                 train_scores_mean + train_scores_std, alpha=0.2,\n",
    "                 color=\"darkorange\", lw=lw)\n",
    "plt.semilogx(param_range, test_scores_mean, label=\"Cross-validation score\",\n",
    "             color=\"navy\", lw=lw)\n",
    "plt.fill_between(param_range, test_scores_mean - test_scores_std,\n",
    "                 test_scores_mean + test_scores_std, alpha=0.2,\n",
    "                 color=\"navy\", lw=lw)\n",
    "plt.legend(loc=\"best\")\n",
    "plt.show()\n",
    "\n",
    "# 3. Plot Confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "cm = confusion_matrix(y_test, y_pred, labels=model.classes_)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=model.classes_)\n",
    "disp.plot(cmap=plt.cm.Blues)\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n",
    "\n",
    "# 4. Plot Accuracy by Class\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(y_test, y_pred, labels=model.classes_)\n",
    "accuracy = np.diag(cm) / np.sum(cm, axis=1)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(model.classes_, accuracy)\n",
    "plt.title(\"Accuracy by Class\")\n",
    "plt"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
